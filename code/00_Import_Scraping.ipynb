{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project 3 Scratchwork Notebook\n",
    "#\n",
    "#\n",
    "#\n",
    "#This notebook crawls the reddit website. I had to adjust my crawling a few times to get the right number of posts, which is why so much of my code is commented out.\n",
    "##\n",
    "##\n",
    "##   WARNING:\n",
    "#    Do not run this notebook all the way through, it will take a long time and has a lot of printouts which I used to track my progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL to the reddit API\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Building a sample pull. Have since learned there is a 100 post limit.\n",
    " params = {\n",
    "     'subreddit':'LifeProTips',\n",
    "     'size':500,\n",
    "     'before':1610463397\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I do a test pull, and verify the creation date of the last post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell I used to get the UTC time of the last post that was pulled.\n",
    "res_2 = requests.get(url, params)\n",
    "\n",
    "data_2 = res_2.json()\n",
    "\n",
    "posts_2 = data_2['data']\n",
    "\n",
    "df_2 = pd.DataFrame(posts_2)\n",
    "\n",
    "df_2.iloc[-1]['created_utc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I decide that a function is the best way to automate the pulling of multiple batches of posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe I should write a function to do this over and over, since I am only getting 100 posts at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and build function:\n",
    "#Goal is to loop through using the 'created_utc' timestamp for n number of iterations, to get 100* that many posts.\n",
    "#This can be used to greatly increase the amount of data we have access to.\n",
    "#Inputs would be a subreddit name, and number of posts by 100 to select. \n",
    "#The function would make all of the requests, and append them to a list or concatenate them to a df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am testing that I can update the before parameter, so I can use it later in my loop to pull and change the date from which I am pulling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subreddit': 'LifeProTips', 'size': 500, 'before': 1000}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = {\n",
    "    'subreddit':'LifeProTips',\n",
    "    'size':500\n",
    "}\n",
    "p.update({'before':1000})\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of data associated with each post. As I want to predict based on the text or title of a post, I want to limit the data I eventually use to 'title' and 'selftext'. I feel the comments, while being able to contribute to the algorithm's ability to determine an ethical post from an unethical one, are not in the spirit of classifying the poster's intent, which is central to the ethicality of the post. I therefore will not use the comments in my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author',\n",
       "       'author_flair_background_color', 'author_flair_css_class',\n",
       "       'author_flair_text', 'author_flair_text_color', 'awarders', 'banned_by',\n",
       "       'can_mod_post', 'contest_mode', 'created_utc', 'domain', 'full_link',\n",
       "       'gildings', 'id', 'is_crosspostable', 'is_meta', 'is_original_content',\n",
       "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
       "       'link_flair_background_color', 'link_flair_richtext',\n",
       "       'link_flair_text_color', 'link_flair_type', 'locked', 'media_only',\n",
       "       'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
       "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
       "       'removed_by_category', 'retrieved_on', 'score', 'send_replies',\n",
       "       'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
       "       'subreddit_subscribers', 'subreddit_type', 'suggested_sort',\n",
       "       'thumbnail', 'title', 'total_awards_received', 'treatment_tags',\n",
       "       'upvote_ratio', 'url', 'whitelist_status', 'wls',\n",
       "       'author_flair_richtext', 'author_flair_type', 'author_fullname',\n",
       "       'author_patreon_flair', 'author_premium', 'link_flair_css_class',\n",
       "       'link_flair_template_id', 'link_flair_text', 'selftext', 'post_hint',\n",
       "       'preview'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is my function, to build a dataframe out of n*100 posts from Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reddit_df(subreddit, before_date, n):\n",
    "    '''\n",
    "    Function loops backwards through a given subreddit, \n",
    "    given a certain starting date, and returns n*100\n",
    "    posts and post text, as well as the url of the post.\n",
    "    #\n",
    "    Inputs:\n",
    "    subreddit (str)  : string name of the subreddit. Must be a valid subreddit. See reddit.com\n",
    "    -\n",
    "    before_date (int): UTC encoded date. Use https://www.epochconverter.com/ if necessary.\n",
    "    -\n",
    "    n (int)          : Number of posts desired, times 100. For example n = 3 will return 300 posts.\n",
    "    -\n",
    "    Returns:\n",
    "    subreddit_df(pd.DataFrame): Returns a dataframe with the n*100 number of posts, post text, and other important\n",
    "    data.\n",
    "    '''\n",
    "    #Create our holder df:\n",
    "    df = pd.DataFrame(columns=['all_awardings', 'allow_live_comments', 'author',\n",
    "       'author_flair_background_color', 'author_flair_css_class',\n",
    "       'author_flair_text', 'author_flair_text_color', 'awarders', 'banned_by',\n",
    "       'can_mod_post', 'contest_mode', 'created_utc', 'domain', 'full_link',\n",
    "       'gildings', 'id', 'is_crosspostable', 'is_meta', 'is_original_content',\n",
    "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
    "       'link_flair_background_color', 'link_flair_richtext',\n",
    "       'link_flair_text_color', 'link_flair_type', 'locked', 'media_only',\n",
    "       'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
    "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
    "       'removed_by_category', 'retrieved_on', 'score', 'send_replies',\n",
    "       'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
    "       'subreddit_subscribers', 'subreddit_type', 'suggested_sort',\n",
    "       'thumbnail', 'title', 'total_awards_received', 'treatment_tags',\n",
    "       'upvote_ratio', 'url', 'whitelist_status', 'wls',\n",
    "       'author_flair_richtext', 'author_flair_type', 'author_fullname',\n",
    "       'author_patreon_flair', 'author_premium', 'link_flair_css_class',\n",
    "       'link_flair_template_id', 'link_flair_text', 'selftext', 'post_hint',\n",
    "       'preview'])\n",
    "    #Declare the base URL of reddit requests location:\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    #Set up the parameters:\n",
    "    params = {\n",
    "        'subreddit':subreddit,\n",
    "        'size':100,\n",
    "        'before':before_date\n",
    "    }\n",
    "    #Loop through n times, grabbing the data we need, and updating the start search term:\n",
    "    for i in range(n):\n",
    "        #Send the request and read in using json function:\n",
    "        res = requests.get(url, params)\n",
    "        try:\n",
    "            data = res.json()\n",
    "            #Data is in the 'data' column, make a temp df from it:\n",
    "            posts = data['data']\n",
    "            post_df = pd.DataFrame(posts)\n",
    "            print(len(df))\n",
    "            df = df.append(post_df)\n",
    "            #Update the before date to the date of the oldest post in the group we just added:\n",
    "            params.update({'before':post_df.iloc[-1]['created_utc']})\n",
    "        except:\n",
    "            #If you encounter an error, skip behind a day, and subtract one from i to make sure you get enough:\n",
    "            params.update({'before':(df.iloc[-1]['created_utc']-86_400)})\n",
    "            i-=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTC date was chosen at random, corresponds to 10PM Jan 12 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is an artifact of the first pulls I did. I have left this code to show my thought process when determining how many posts to pull:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################DO NOT UNCOMMENT################################################\n",
    "#Ok we need to look at these posts, and see if any of them have been removed or are nan, and go back for more posts \n",
    "\n",
    "#if there aren't enough full posts.\n",
    "\n",
    "#lpt_df[['selftext', 'title']].isna().value_counts()\n",
    "\n",
    "#ulpt_df[['selftext', 'title']].isna().value_counts()\n",
    "\n",
    "#Not too bad, lets drop the Nans.\n",
    "# lpt_df.dropna(subset = ['selftext', 'title'], inplace=True)\n",
    "# len(lpt_df)\n",
    "\n",
    "\n",
    "# ulpt_df.dropna(subset = ['selftext', 'title'], inplace=True)\n",
    "# len(ulpt_df)\n",
    "\n",
    "#lpt_df[['selftext', 'title']].iloc[3][0]\n",
    "\n",
    "#Ok lets see how much we would be left with when we drop the ['removed'] posts:\n",
    "#len(lpt_df[lpt_df['selftext'] != '[removed]'])\n",
    "\n",
    "#len(ulpt_df[ulpt_df['selftext'] != '[removed]'])\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result, I need to pull 20_000 to 22_000 posts to get the number of posts I would like to have in the end (about 7500 posts each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have commented the cell out below, so it is not run in error. If you wish to run the cell, it will count out in 100s how many posts it has pulled.\n",
    "This is how I tracked the progress and verified it was working as intended while I went and did other work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n"
     ]
    }
   ],
   "source": [
    "# # #Looks like we need to double our crawling.\n",
    "# lpt_df_2 = build_reddit_df('LifeProTips', 1610427600, 200)\n",
    "# ulpt_df_2 = build_reddit_df('UnethicalLifeProTips', 1610427600, 220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lpt_df_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0fdfe1b08aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlpt_df_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mulpt_df_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lpt_df_2' is not defined"
     ]
    }
   ],
   "source": [
    "#drop the nans.\n",
    "lpt_df_2.dropna(subset = ['selftext', 'title'], inplace=True)\n",
    "ulpt_df_2.dropna(subset = ['selftext', 'title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the dataframes to csv\n",
    "lpt_df_2.to_csv('./data/lpt_df_2.csv')\n",
    "ulpt_df_2.to_csv('./data/ulpt_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. Data is scraped from reddit, we have removed the Nans, and will remove posts that are not applicable ('removed' posts) in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
